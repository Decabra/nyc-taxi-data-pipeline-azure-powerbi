{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "356bf9e7-4442-49d1-a98d-5c790e80c352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in c:\\users\\sarma\\anaconda3\\lib\\site-packages (2024.8.2)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (16.1.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from click>=8.1->dask[complete]) (0.4.6)\n",
      "Requirement already satisfied: locket in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from pyarrow>=14.0.1->dask[complete]) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (2.2.2)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (1.1.13)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (3.6.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (3.1.4)\n",
      "Requirement already satisfied: distributed==2024.8.2 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from dask[complete]) (2024.8.2)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (5.9.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (2.2.3)\n",
      "Requirement already satisfied: zict>=3.0.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from distributed==2024.8.2->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.2 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from bokeh>=2.4.2->dask[complete]) (1.2.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from bokeh>=2.4.2->dask[complete]) (10.4.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from bokeh>=2.4.2->dask[complete]) (2022.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas sqlalchemy pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d1811d-0618-4ba8-a6da-d5e565f4a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  glob, os, pandas as pd\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9919f3fb-ee58-49a0-a066-1f21673e636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected successfully. Server time: (datetime.datetime(2025, 5, 29, 18, 8, 27, 393000),)\n"
     ]
    }
   ],
   "source": [
    "# Credentials\n",
    "server = 'sqlmsazure.database.windows.net'\n",
    "database = 'taxis'\n",
    "username = 'sarmadcute83'\n",
    "password = 'Azurepowerbi@123'  # Replace with your actual password\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# Encode special characters in password (e.g., @ = %40)\n",
    "password = password.replace('@', '%40')\n",
    "\n",
    "# Build connection string\n",
    "connection_string = (\n",
    "    f\"mssql+pyodbc://{username}:{password}@{server}:1433/{database}\"\n",
    "    f\"?driver={driver.replace(' ', '+')}\"\n",
    ")\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "# Test connection\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT GETDATE();\"))\n",
    "    print(\"✅ Connected successfully. Server time:\", result.fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be255411-2eb8-4c33-a5ab-abd16322a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/sarma/Power BI/NYC Yellow Taxi/dataset\"\n",
    "files_2024 = glob.glob(os.path.join(folder_path, 'yellow_tripdata_2024-*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d08887e-0bfc-4086-8dbf-3352357f68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2023 = glob.glob(os.path.join(folder_path, 'yellow_tripdata_2023-*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38536895-2cde-491e-b7c4-aef94dd83c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023 = pd.concat([pd.read_parquet(file) for file in files_2023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661b4f46-ef0c-4c95-91db-a51791c680c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = pd.concat([pd.read_parquet(file) for file in files_2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c38f3d-3e1d-4a75-b19f-f5b8397ee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge airport fee columns based on which one has the values\n",
    "df_2023['Airport_fee'] = df_2023['airport_fee'].combine_first(df_2023['Airport_fee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33994fe4-dd3f-4b5c-828e-48ca5c111de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop the extra airport fee column\n",
    "df_2023.drop(columns = ['airport_fee'],  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36331302-64fe-4760-938e-11078ee36961",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df_2023, df_2024], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43105e8b-4dd6-4fb2-85fc-8233179ab84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'tpep_pickup_datetime', # The date and time when the meter was engaged.\n",
    "    'tpep_dropoff_datetime', # The date and time when the meter was disengaged.\n",
    "    'passenger_count', # The number of passengers in the vehicle.\n",
    "    'trip_distance', # The elapsed trip distance in miles reported by the taximeter.\n",
    "    'PULocationID', # TLC Taxi Zone in which the taximeter was engaged\n",
    "    'DOLocationID', # TLC Taxi Zone in which the taximeter was disengaged\n",
    "    'payment_type', # 0 = Flex Fare trip 1 = Credit card 2 = Cash 3 = No charge 4 = Dispute 5 = Unknown 6 = Voided trip\n",
    "    'fare_amount', # The time-and-distance fare calculated by the meter.\n",
    "    'extra', # Miscellaneous extras and surcharges.\n",
    "    'mta_tax', # Tax that is automatically triggered based on the metered rate in use.\n",
    "    'tip_amount', # Tip amount – This field is automatically populated for credit card tips. Cash tips are not included.\n",
    "    'tolls_amount', # Total amount of all tolls paid in trip.\n",
    "    'total_amount', # The total amount charged to passengers. Does not include cash tips.\n",
    "    'congestion_surcharge', # Total amount collected in trip for NYS congestion surcharge.\n",
    "    'Airport_fee', # For pick up only at LaGuardia and John F. Kennedy Airports.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b473714-c818-4677-a4b8-5498a430674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis_df = final_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5d7e559-8ec4-4b74-b869-b45d98f5c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a record_id to match with our SQL table\n",
    "taxis_df = taxis_df.copy() # Fix copy of a slice warning from pandas\n",
    "taxis_df.reset_index(drop=True, inplace=True)\n",
    "taxis_df['record_id'] = taxis_df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4fa0f7d-a358-45d3-9074-73f88114789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79479946, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxis_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b3457-b3fb-43ba-8718-3876b76e7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk 24095000-24295000 to staging table\n",
      "MERGE successful for rows 24095000–24295000\n",
      "\n",
      "Uploaded chunk 24295000-24495000 to staging table\n",
      "MERGE successful for rows 24295000–24495000\n",
      "\n",
      "Uploaded chunk 24495000-24695000 to staging table\n",
      "MERGE successful for rows 24495000–24695000\n",
      "\n",
      "Uploaded chunk 24695000-24895000 to staging table\n",
      "MERGE successful for rows 24695000–24895000\n",
      "\n",
      "Uploaded chunk 24895000-25095000 to staging table\n",
      "MERGE successful for rows 24895000–25095000\n",
      "\n",
      "Uploaded chunk 25095000-25295000 to staging table\n",
      "MERGE successful for rows 25095000–25295000\n",
      "\n",
      "Uploaded chunk 25295000-25495000 to staging table\n",
      "MERGE successful for rows 25295000–25495000\n",
      "\n",
      "Uploaded chunk 25495000-25695000 to staging table\n",
      "MERGE successful for rows 25495000–25695000\n",
      "\n",
      "Uploaded chunk 25695000-25895000 to staging table\n",
      "MERGE successful for rows 25695000–25895000\n",
      "\n",
      "Uploaded chunk 25895000-26095000 to staging table\n",
      "MERGE successful for rows 25895000–26095000\n",
      "\n",
      "Uploaded chunk 26095000-26295000 to staging table\n",
      "MERGE successful for rows 26095000–26295000\n",
      "\n",
      "Uploaded chunk 26295000-26495000 to staging table\n",
      "MERGE successful for rows 26295000–26495000\n",
      "\n",
      "Uploaded chunk 26495000-26695000 to staging table\n",
      "MERGE successful for rows 26495000–26695000\n",
      "\n",
      "Uploaded chunk 26695000-26895000 to staging table\n",
      "MERGE successful for rows 26695000–26895000\n",
      "\n",
      "Uploaded chunk 26895000-27095000 to staging table\n",
      "MERGE successful for rows 26895000–27095000\n",
      "\n",
      "Uploaded chunk 27095000-27295000 to staging table\n",
      "MERGE successful for rows 27095000–27295000\n",
      "\n",
      "Uploaded chunk 27295000-27495000 to staging table\n",
      "MERGE successful for rows 27295000–27495000\n",
      "\n",
      "Uploaded chunk 27495000-27695000 to staging table\n",
      "MERGE successful for rows 27495000–27695000\n",
      "\n",
      "Uploaded chunk 27695000-27895000 to staging table\n",
      "MERGE successful for rows 27695000–27895000\n",
      "\n",
      "Uploaded chunk 27895000-28095000 to staging table\n",
      "MERGE successful for rows 27895000–28095000\n",
      "\n",
      "Uploaded chunk 28095000-28295000 to staging table\n",
      "MERGE successful for rows 28095000–28295000\n",
      "\n",
      "Uploaded chunk 28295000-28495000 to staging table\n",
      "MERGE successful for rows 28295000–28495000\n",
      "\n",
      "Uploaded chunk 28495000-28695000 to staging table\n",
      "MERGE successful for rows 28495000–28695000\n",
      "\n",
      "Uploaded chunk 28695000-28895000 to staging table\n",
      "MERGE successful for rows 28695000–28895000\n",
      "\n",
      "Uploaded chunk 28895000-29095000 to staging table\n",
      "MERGE successful for rows 28895000–29095000\n",
      "\n",
      "Uploaded chunk 29095000-29295000 to staging table\n",
      "MERGE successful for rows 29095000–29295000\n",
      "\n",
      "Uploaded chunk 29295000-29495000 to staging table\n",
      "MERGE successful for rows 29295000–29495000\n",
      "\n",
      "Uploaded chunk 29495000-29695000 to staging table\n",
      "MERGE successful for rows 29495000–29695000\n",
      "\n",
      "Uploaded chunk 29695000-29895000 to staging table\n",
      "MERGE successful for rows 29695000–29895000\n",
      "\n",
      "Uploaded chunk 29895000-30095000 to staging table\n",
      "MERGE successful for rows 29895000–30095000\n",
      "\n",
      "Uploaded chunk 30095000-30295000 to staging table\n",
      "MERGE successful for rows 30095000–30295000\n",
      "\n",
      "Uploaded chunk 30295000-30495000 to staging table\n",
      "MERGE successful for rows 30295000–30495000\n",
      "\n",
      "Uploaded chunk 30495000-30695000 to staging table\n",
      "MERGE successful for rows 30495000–30695000\n",
      "\n",
      "Uploaded chunk 30695000-30895000 to staging table\n",
      "MERGE successful for rows 30695000–30895000\n",
      "\n",
      "Uploaded chunk 30895000-31095000 to staging table\n",
      "MERGE successful for rows 30895000–31095000\n",
      "\n",
      "Uploaded chunk 31095000-31295000 to staging table\n",
      "MERGE successful for rows 31095000–31295000\n",
      "\n",
      "Uploaded chunk 31295000-31495000 to staging table\n",
      "MERGE successful for rows 31295000–31495000\n",
      "\n",
      "Uploaded chunk 31495000-31695000 to staging table\n",
      "MERGE successful for rows 31495000–31695000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = 'yellow_tripdata_tmp'\n",
    "staging_table = 'staging_chunk'\n",
    "chunk_size = 200_000\n",
    "total_rows = len(taxis_df)\n",
    "checkpoint_file = 'upload_checkpoint.txt'\n",
    "start = 0\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        start = int(f.read().strip())\n",
    "\n",
    "while start < total_rows: \n",
    "    end = min(start + chunk_size, total_rows)\n",
    "    chunk = taxis_df.iloc[start:end]\n",
    "    try:\n",
    "        chunk.to_sql(\n",
    "            staging_table,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method=None\n",
    "        )\n",
    "        print(f\"Uploaded chunk {start}-{end} to staging table\")\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE {table_name} AS target\n",
    "        USING {staging_table} AS source\n",
    "        ON target.record_id = source.record_id\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                target.tpep_pickup_datetime = source.tpep_pickup_datetime,\n",
    "                target.tpep_dropoff_datetime = source.tpep_dropoff_datetime,\n",
    "                target.passenger_count = source.passenger_count,\n",
    "                target.trip_distance = source.trip_distance,\n",
    "                target.PULocationID = source.PULocationID,\n",
    "                target.DOLocationID = source.DOLocationID,\n",
    "                target.payment_type = source.payment_type,\n",
    "                target.fare_amount = source.fare_amount,\n",
    "                target.extra = source.extra,\n",
    "                target.mta_tax = source.mta_tax,\n",
    "                target.tip_amount = source.tip_amount,\n",
    "                target.tolls_amount = source.tolls_amount,\n",
    "                target.total_amount = source.total_amount,\n",
    "                target.congestion_surcharge = source.congestion_surcharge,\n",
    "                target.Airport_fee = source.Airport_fee,\n",
    "                target.updated_at = GETDATE()\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (\n",
    "                record_id,\n",
    "                tpep_pickup_datetime,\n",
    "                tpep_dropoff_datetime,\n",
    "                passenger_count,\n",
    "                trip_distance,\n",
    "                PULocationID,\n",
    "                DOLocationID,\n",
    "                payment_type,\n",
    "                fare_amount,\n",
    "                extra,\n",
    "                mta_tax,\n",
    "                tip_amount,\n",
    "                tolls_amount,\n",
    "                total_amount,\n",
    "                congestion_surcharge,\n",
    "                Airport_fee,\n",
    "                created_at\n",
    "            )\n",
    "            VALUES (\n",
    "                source.record_id,\n",
    "                source.tpep_pickup_datetime,\n",
    "                source.tpep_dropoff_datetime,\n",
    "                source.passenger_count,\n",
    "                source.trip_distance,\n",
    "                source.PULocationID,\n",
    "                source.DOLocationID,\n",
    "                source.payment_type,\n",
    "                source.fare_amount,\n",
    "                source.extra,\n",
    "                source.mta_tax,\n",
    "                source.tip_amount,\n",
    "                source.tolls_amount,\n",
    "                source.total_amount,\n",
    "                source.congestion_surcharge,\n",
    "                source.Airport_fee,\n",
    "                GETDATE()\n",
    "            );\n",
    "        \"\"\"\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(merge_sql)) # Merge record into tmp table\n",
    "            conn.execute(text(\"TRUNCATE TABLE staging_chunk;\")) # Clear the staging table for the next chunk\n",
    "        \n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            f.write(str(end))\n",
    "        \n",
    "        print(f\"MERGE successful for rows {start}–{end}\\n\")\n",
    "        start = end\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunk {start}–{end}: {e}\")\n",
    "        print(\"Stopping upload to avoid duplicate or partial inserts.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
